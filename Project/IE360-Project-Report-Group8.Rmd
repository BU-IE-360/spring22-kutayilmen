---
title: "IE360 Project Report Group8"
author: "Mustafa Comert, Fatih Mehmet Yilmaz and Kutay Ilmen Aytekin"
date: "07/06/2022"
output: 
  html_document :
    code_folding : "hide"
---

```{r Data Manipulation, warning=FALSE, message=FALSE, include=FALSE}
library(data.table)
library(forecast)
library(ggplot2)
library(reshape2)
library(lubridate)
library(knitr)
library(urca)
library(tidyverse)

todays_date=as.Date(today())
forecast_date=todays_date+1
production=fread("2022-06-07_production.csv")
weather=fread("2022-06-07_weather.csv")

# identify # of days to forecast
latest_available_prod_date=as.Date(max(production$date))
n_days=as.numeric(forecast_date-latest_available_prod_date)

# get the latest n_days and modify for forecasting
# creates date,hour,production for the upcoming days
forecasted_production=tail(production,n_days*24)
forecasted_production[,date:=date+n_days]
forecasted_production[,production:=NA]

# actual production data with forecasted dates
production_with_forecast=rbind(production,forecasted_production)

# create a template for forecast date
forecast_table=data.table(date=forecast_date,hour=0:23,production=NA)

# transform long weather to wide
wide_weather=dcast(weather,date+hour~variable+lat+lon,value.var='value')

# merge with production with forecast dates
production_with_weather=merge(production_with_forecast,wide_weather,by=c('date','hour'))

# to account for hourly seasonality in regression setting, define hour as character
production_with_weather[,hour:=as.factor(hour)]

#reporting accuracy
statistics <- function(actual, forecasted){
  n=length(actual)
  error = actual - forecasted
  mean = mean(actual)
  sd = sd(actual)
  bias = sum(error) / sum(actual)
  mad = sum(abs(error)) / n
  wmape = mad / mean
  l = data.frame(n, mean, sd, bias, mad, wmape)
  colnames(l) <- c("N", "Mean", "Standard Deviation", "Bias", "MAD", "WMAPE")
  return(l)
}

```

# 1 - Introduction

In this project, we are asked to predict hourly solar power production of KIVANC 2 GES between May 25 - June 3, 2022. Every prediction consists of 24 hours of the next day. The available data used in prediction process include production information of previous day, and independent weather variables such as temperature, relative humidity, downward shortwave radiation flux and cloud cover. Weather variables are given for 9 coordinates around solar power plant.

Firstly, it is a good idea to check our data visually before constructing prediction models. Below is the plot of the daily total production for available data to examine the general behavior of the production.

```{r Daily Total Production, warning=FALSE, message=FALSE}
#convert hourly data to daily data
daily_data=production_with_weather[,list(total_p=sum(production),max_p=max(production),
                      mean_p=mean(production)),list(date)]

ggplot(daily_data ,aes(x=date,y=total_p)) +
  geom_line(color = "darkred") +
  labs(title = "Daily Total Production for Available Data",
       x = "Date",
       y= "Production (MWh)" ) +
  scale_x_date(date_breaks = "3 month", date_labels = "%d %b %y") +
  theme_minimal()
```

After checking the daily total production plot, it can be observed that the daily total productions fluctuate in consecutive days that might be due to changing weather conditions. After the interpretations, it is logical to add independent weather variables into our models.

Next plot is for daily maximum production for available data.

```{r Daily Maximum Production, warning=FALSE, message=FALSE}
#Daily Maximum Production for Available Data
ggplot(daily_data ,aes(x=date,y=max_p)) + 
  geom_line(color = "darkred") +
  labs(title = "Daily Maximum Production for Available Data",
       x = "Date",
       y= "Production (MWh)" ) +
  scale_x_date(date_breaks = "3 month", date_labels = "%d %b %y") +
  theme_minimal()
```

In this plot, it can be seen that production capacity changes over time. Especially, after the setup of the power plant there is an increase in the capacity until summer of 2021. Toward the next winter, the capacity decreased to 35 MWh. The interesting part is that in our prediction period, the capacity started to increase again. This capacity will be used in our prediction models. We can also comment that the capacity depends on the weather conditions similar to daily total production. With this result, the production will be normalized according to capacity and added into our model. In our trials, we checked both the models with actual production and normalized production. We concluded to use normalized production in the model since it gave better statistics.

Another plot is the daily total production for a month to see weekly behaviour. The period for this plot is selected from a constant capacity time.

```{r Daily Total Production for a Month, warning=FALSE, message=FALSE}
# daily plot
ggplot(data = daily_data[which(date=='2021-07-01'):which(date=='2021-07-28')], aes(x = date, y = total_p)) +
  geom_line(color = "darkred") +
  labs(title = "Daily Total Production Data between 01/07/21 and 28/07/21 ",
       x = "Date",
       y= "Production (MWh)" ) +
  scale_x_date(date_breaks = "7 days", date_labels = "%d %b %y") +
  theme_minimal()
```

After checking the, we concluded that there is no weekly seasonality. However we need to check hourly production to check whether there is daily seasonality or not, as well. 

Next is the hourly production selected from the first 10 days of the available data.

```{r Hourly Production for 10 Days, warning=FALSE, message=FALSE}
copy <- production_with_weather
copy$hour <- as.numeric(copy$hour)
copy[,datetime:=ymd(date)+dhours(hour)]
copy=copy[order(datetime)]
# hourly plot
ggplot(data = copy[1:240], aes(x = datetime, y = production)) +
  geom_line(color = "darkred") +
  labs(title = "Daily Consumption Data between 01/02/21 and 10/02/21 ",
       x = "Date",
       y= "Consumption (MWh)")  
```

It is clear that, there is a daily seasonality in the data. This result is aligned with our intuitions coming from the existence of the daylight. With this result, we add hourly seasonality into our models. 
Since the locations are next to each other, we wanted check each location's correlation with the production to check if they are similar.


```{r Initial Models, warning=FALSE, message=FALSE, include=FALSE}
# test and train phases
production_with_weather[,hour:=as.factor(hour)]
train_data=production_with_weather[!is.na(production)]
test_data=production_with_weather[is.na(production)]

# to get last week of the data contains actual production
march_first <- which(grepl('2022-03-01',train_data$date) & grepl(0,train_data$hour) & !grepl(1,train_data$hour) & !grepl(2,train_data$hour))

may_twentyfourth <- which(grepl('2022-05-25',train_data$date) & grepl(0,train_data$hour) & !grepl(1,train_data$hour) & !grepl(2,train_data$hour))

evaluation_period <- march_first:(may_twentyfourth-1)

# control phase to check the statistics of the model
control_data=production_with_weather[evaluation_period]
control_data[,datetime:=as.POSIXct(paste(control_data$date, control_data$hour), format="%Y-%m-%d %H")]
control_data=control_data[order(datetime)]

control_data_1week=train_data[(nrow(train_data)-167):nrow(train_data)]
control_data_1week[,datetime:=as.POSIXct(paste(control_data_1week$date, control_data_1week$hour), format="%Y-%m-%d %H")]
control_data_1week=control_data_1week[order(datetime)]

# Model 1 with all independent variables
# train with all variables
lm_model1=lm(production~.,train_data[,-c('date')])
summary(lm_model1)
#checkresiduals(lm_model1)
lm_forecast_test=predict(lm_model1,test_data)
test_data[,forecasted1:=as.numeric(lm_forecast_test)]
lm_forecast_control=predict(lm_model1,control_data)
control_data[,forecasted1:=as.numeric(lm_forecast_control)]
# for 7 days control
lm_forecast_control=predict(lm_model1,control_data_1week)
control_data_1week[,forecasted1:=as.numeric(lm_forecast_control)]

# postprocess the forecast based on actual production
control_data[production<=0,'forecasted1':=0]
control_data[forecasted1<=0,'forecasted1':=0]
control_data[production>=max(production),'forecasted1':=max(production)]
control_data[forecasted1>=max(production),'forecasted1':=max(production)]

control_data_1week[production<=0,'forecasted1':=0]
control_data_1week[forecasted1<=0,'forecasted1':=0]
control_data_1week[production>=max(production),'forecasted1':=max(production)]
control_data_1week[forecasted1>=max(production),'forecasted1':=max(production)]

# update forecast table
forecast_table[,forecast_all_var:=test_data[date==(forecast_date)]$forecasted1]

# check statistics of model 1
kable(statistics(control_data$production, control_data$forecasted1),
      caption = "Statistics of Model 1 ", align = 'c')

```

``` {r Initial Models_correlation, warning=FALSE, message=FALSE}
#----------------Model 2-------------------
# check the correlations
vec <- as.matrix(cor(train_data[,c(4:39)], train_data$production))
mat <- matrix(vec,nrow=9)
mat <- as.data.table(mat)
colnames(mat) <- c("Cloud Cover", "Flux", "Humidity", "Temp")
mat
```

The result shows us that, each location has a similar correlation with the production. Therefore, instead of using each location in the model, we decided to use their average values as solely predictors for weather variables. In our model trials, we checked each case separately (average values and separate coordination values), and decided to continue with average values.


```{r Initial Models_2, warning=FALSE, message=FALSE, include=FALSE}
# take means of variables
cloud_mean <- rowMeans(subset(production_with_weather,select = c(4:12)))
flux_mean <- rowMeans(subset(production_with_weather,select = c(13:21)))  
humidity_mean <- rowMeans(subset(production_with_weather,select = c(22:30)))
temp_mean <- rowMeans(subset(production_with_weather,select = c(31:39)))

production_with_weather_means = subset(production_with_weather, select = -c(4 : 39))

production_with_weather_means[,cloud_mean:=NA]
production_with_weather_means[,flux_mean:=NA]
production_with_weather_means[,humidity_mean:=NA]
production_with_weather_means[,temp_mean:=NA]

for(i in 1:nrow(production_with_weather_means)){
  production_with_weather_means$cloud_mean[i] = cloud_mean[i]
  production_with_weather_means$flux_mean[i] = flux_mean[i]
  production_with_weather_means$humidity_mean[i] = humidity_mean[i]
  production_with_weather_means$temp_mean[i] = temp_mean[i]  
}

# test and train phases
production_with_weather_means[,hour:=as.factor(hour)]
train_data_means=production_with_weather_means[!is.na(production)]
test_data_means=production_with_weather_means[is.na(production)]

# control phase to check the statistics of the model
control_data_means=production_with_weather_means[evaluation_period]
control_data_means[,datetime:=as.POSIXct(paste(control_data_means$date, control_data_means$hour), format="%Y-%m-%d %H")]
control_data_means=control_data_means[order(datetime)]

control_data_1week_means=train_data_means[(nrow(train_data_means)-167):nrow(train_data_means)]
control_data_1week_means[,datetime:=as.POSIXct(paste(control_data_1week_means$date, control_data_1week_means$hour), format="%Y-%m-%d %H")]
control_data_1week_means=control_data_1week_means[order(datetime)]

# model 2 with means 
lm_model2 <- lm(production~., train_data_means[,-c('date')])
summary(lm_model2)
#checkresiduals(lm_model2)
lm_forecast_test=predict(lm_model2,test_data_means)
test_data_means[,forecasted2:=as.numeric(lm_forecast_test)]
lm_forecast_control=predict(lm_model2,control_data_means)
control_data_means[,forecasted2:=as.numeric(lm_forecast_control)]

# for 7 days control
lm_forecast_control=predict(lm_model2,control_data_1week_means)
control_data_1week_means[,forecasted2:=as.numeric(lm_forecast_control)]

# postprocess the forecast based on actual production
control_data_means[production<=0,'forecasted2':=0]
control_data_means[forecasted2<=0,'forecasted2':=0]
control_data_means[production>=max(production),'forecasted2':=max(production)]
control_data_means[forecasted2>=max(production),'forecasted2':=max(production)]

control_data_1week_means[production<=0,'forecasted2':=0]
control_data_1week_means[forecasted2<=0,'forecasted2':=0]
control_data_1week_means[production>=max(production),'forecasted2':=max(production)]
control_data_1week_means[forecasted2>=max(production),'forecasted2':=max(production)]

# update forecast table
forecast_table[,forecast_with_means:=test_data_means[date==forecast_date]$forecasted2]

# check statistics of model 2
kable(statistics(control_data_means$production, control_data_means$forecasted2),
      caption = "Statistics of Model 1 ", align = 'c')
```

# 2 - Related Literature

In this project, a set of independent variables are given with the production data of solar power production. It is known that high temperatures affect the solar panels and decrease their efficiency. Therefore, we take this proposal as known and use the temperature variables in our models. Also, we use relative humidity values of different locations near the solar power plant and it is expected that the increase in relative humidity decreases the solar [power production][humidity]. The other variable that is used in this project is downward shortwave radiation flux. It is known that it is highly related to the production level. We also use the total cloud cover (in terms of percentage) variables in our models. It is logically related to [solar power production][cloud].It is clear from the study that the cloud cover is inversely proportional to the output performance of the solar power production. These studies show the behaviors of the variables and we only use this information to check whether the variables have coefficients accordingly. Therefore, we did spend much time to understand the variables clearly, and we follow a data-driven approach and use our insights to validate our models.   
   

# 3 - Approach

A base model including the averages of weather variables, hourly seasonality and normalized production according to capacity is constructed according to information given above. To improve the model, some adjustments will be made in this section.


## Base Model

The base model is as follows:

```{r Base Model, warning=FALSE, message=FALSE}
daily_max_production=production_with_forecast[,list(max_prod=max(production)),by=list(date)]
daily_max_production[,rolling_max:=frollapply(max_prod,30,max,na.rm=T)]

# due to the loss of information, we used the daily max for each day for the first 30 days
daily_max_production$rolling_max[1:29] = frollapply(daily_max_production$max_prod[1:29],5,max,na.rm=T)
daily_max_production$rolling_max[1:4] <- as.double(max(daily_max_production$max_prod[1:4])) 

# merge with regression data
production_with_weather_means_capacity=merge(production_with_weather_means,daily_max_production,by=c('date'))
production_with_weather_means_capacity[,normalized_production:=production/rolling_max]

# test and train phases
production_with_weather_means_capacity[,hour:=as.factor(hour)]
train_data_means_capacity=production_with_weather_means_capacity[!is.na(production)]
test_data_means_capacity=production_with_weather_means_capacity[is.na(production)]

# control phase to check the statistics of the model
control_data_means_capacity=production_with_weather_means_capacity[evaluation_period]
control_data_means_capacity[,datetime:=as.POSIXct(paste(control_data_means_capacity$date, control_data_means_capacity$hour), format="%Y-%m-%d %H")]
control_data_means_capacity=control_data_means_capacity[order(datetime)]

# for a week control phase 
control_data_1week_means_capacity=train_data_means_capacity[(nrow(train_data_means_capacity)-167):nrow(train_data_means_capacity)]
control_data_1week_means_capacity[,datetime:=as.POSIXct(paste(control_data_1week_means_capacity$date, control_data_1week_means_capacity$hour), format="%Y-%m-%d %H")]
control_data_1week_means_capacity=control_data_1week_means_capacity[order(datetime)]


# model 3 with daily max in a moving manner to normalize with means of locations
lm_model3=lm(normalized_production~.,train_data_means_capacity[,-c('date','rolling_max','production','max_prod'),with=F])
summary(lm_model3)
checkresiduals(lm_model3)
lm_forecast_test=predict(lm_model3,test_data_means_capacity)
test_data_means_capacity[,forecasted3:=as.numeric(lm_forecast_test)*rolling_max]
lm_forecast_control=predict(lm_model3,control_data_means_capacity)
control_data_means_capacity[,forecasted3:=as.numeric(lm_forecast_control)*rolling_max]
# for 7 days control
lm_forecast_control=predict(lm_model3,control_data_1week_means_capacity)
control_data_1week_means_capacity[,forecasted3:=as.numeric(lm_forecast_control)*rolling_max]

# postprocess the forecast based on actual production
control_data_means_capacity[production<=0,'forecasted3':=0]
control_data_means_capacity[forecasted3<=0,'forecasted3':=0]
control_data_means_capacity[production>=max(production),'forecasted3':=max(production)]
control_data_means_capacity[forecasted3>=max(production),'forecasted3':=max(production)]

control_data_1week_means_capacity[production<=0,'forecasted3':=0]
control_data_1week_means_capacity[forecasted3<=0,'forecasted3':=0]
control_data_1week_means_capacity[production>=max(production),'forecasted3':=max(production)]
control_data_1week_means_capacity[forecasted3>=max(production),'forecasted3':=max(production)]

# update forecast table
forecast_table[,forecast_with_means_normalized:=test_data_means_capacity[date==forecast_date]$forecasted3]

```

In the base model's output, it can be seen that hourly variables are statistically significant in the day time, independent variables are also significant except for temperature. The reason for an insignificant temperature might be it is already explained by other variables. The residual standard error of the base model is 0.152 and this value will be compared with other model. The adjusted R-squared value is 0.854 which can be improved. After checking the residuals, it can be observed that there is a high autocorrelation between residuals at specific lag of 24. The normality assumption also does not hold. Lastly, the stationarity assumption does not hold with respect to variance. The variance of residuals tend to be lower on the summer months. 

## Model Adjustment 1

To improve the model, a lag variable will be added into the model. Ideally lag at 24 would be suitable however considering the available data and prediction task, lag at 72 will be added into the model.

```{r warning=FALSE, message=FALSE}
lag72 <- vector()
train_data_means_capacity= cbind(train_data_means_capacity, lag72)
train_data_means_capacity$lag72[1:72] = NA
train_data_means_capacity$lag72[73:nrow(train_data_means_capacity)] = train_data_means_capacity$production[1:(nrow(train_data_means_capacity)-72)]
train_data_means_capacity = train_data_means_capacity[!is.na(lag72)]

test_data_means_capacity= cbind(test_data_means_capacity, lag72)
min = nrow(train_data_means_capacity) - 71
max = nrow(train_data_means_capacity)
test_data_means_capacity$lag72 = train_data_means_capacity$production[c(min:max)]

#control_data_means_capacity <- train_data_means_capacity[evaluation_period]
control_data_means_capacity= cbind(control_data_means_capacity, lag72)
control_data_means_capacity$lag72 = train_data_means_capacity$lag72[evaluation_period - 72]

control_data_1week_means_capacity= cbind(control_data_1week_means_capacity, lag72)
control_data_1week_means_capacity$lag72 = train_data_means_capacity$lag72[(nrow(train_data_means_capacity)-167):nrow(train_data_means_capacity)]

# train with all variables
lm_model5=lm(normalized_production~.,train_data_means_capacity[,-c('date','rolling_max','production','max_prod'),with=F])
summary(lm_model5)
checkresiduals(lm_model5)
lm_forecast_test=predict(lm_model5,test_data_means_capacity)
test_data_means_capacity[,forecasted5:=as.numeric(lm_forecast_test)*rolling_max]
lm_forecast_control=predict(lm_model5,control_data_means_capacity)
control_data_means_capacity[,forecasted5:=as.numeric(lm_forecast_control)*rolling_max]
#for 1 week
lm_forecast_control=predict(lm_model5,control_data_1week_means_capacity)
control_data_1week_means_capacity[,forecasted5:=as.numeric(lm_forecast_control)*rolling_max]

# postprocess the forecast based on actual production
control_data_means_capacity[production<=0,'forecasted5':=0]
control_data_means_capacity[forecasted5<=0,'forecasted5':=0]
control_data_means_capacity[production>=max(production),'forecasted5':=max(production)]
control_data_means_capacity[forecasted5>=max(production),'forecasted5':=max(production)]

control_data_1week_means_capacity[production<=0,'forecasted5':=0]
control_data_1week_means_capacity[forecasted5<=0,'forecasted5':=0]
control_data_1week_means_capacity[production>=max(production),'forecasted5':=max(production)]
control_data_1week_means_capacity[forecasted5>=max(production),'forecasted5':=max(production)]

# update forecast table
forecast_table[,forecast_with_means_normalized_lag72:=test_data_means_capacity[date==forecast_date]$forecasted5]

```

With the addition of lag72 variable, our residual standard error decreased to 0.1438 and adjusted R-squared valued increased to 0.8698. In this new model, lag72 variable is significant as expected and temperature variable became significant as well. Autocorrelation decreased to some extent however it still exist. Other assumptions which are stationarity of the variance and normality of the residual do not hold.

## Model Adjustment 2

Next, in order to handle serial dependence difference of the consecutive observations will be added to the model. 

```{r warning=FALSE, message=FALSE}

train_data_means_capacity[,differ1:=train_data_means_capacity$production-shift(train_data_means_capacity$production,1)]
test_data_means_capacity[,differ1:=train_data_means_capacity$production[c(min:max)]-shift(train_data_means_capacity$production[c(min:max)],1)]
train_data_means_capacity$differ1[1] = 0
test_data_means_capacity$differ1[1] = 0

differ1 <- vector()
control_data_means_capacity <- cbind(control_data_means_capacity, differ1)
control_data_means_capacity$differ1 <- train_data_means_capacity$differ1[evaluation_period-72]

control_data_1week_means_capacity <- cbind(control_data_1week_means_capacity, differ1)
control_data_1week_means_capacity$differ1 <- train_data_means_capacity$differ1[(nrow(train_data_means_capacity)-167):nrow(train_data_means_capacity)]

lm_model6=lm(normalized_production~.,train_data_means_capacity[,-c('date','rolling_max','production','max_prod'),with=F])
summary(lm_model6)
checkresiduals(lm_model6)
lm_forecast_test=predict(lm_model6,test_data_means_capacity)
test_data_means_capacity[,forecasted6:=as.numeric(lm_forecast_test)*rolling_max]
lm_forecast_control=predict(lm_model6,control_data_means_capacity)
control_data_means_capacity[,forecasted6:=as.numeric(lm_forecast_control)*rolling_max]
# for 1 week
lm_forecast_control=predict(lm_model6,control_data_1week_means_capacity)
control_data_1week_means_capacity[,forecasted6:=as.numeric(lm_forecast_control)*rolling_max]

# postprocess the forecast based on actual production
control_data_means_capacity[production<=0,'forecasted6':=0]
control_data_means_capacity[forecasted6<=0,'forecasted6':=0]
control_data_means_capacity[production>=max(production),'forecasted6':=max(production)]
control_data_means_capacity[forecasted6>=max(production),'forecasted6':=max(production)]

control_data_1week_means_capacity[production<=0,'forecasted6':=0]
control_data_1week_means_capacity[forecasted6<=0,'forecasted6':=0]
control_data_1week_means_capacity[production>=max(production),'forecasted6':=max(production)]
control_data_1week_means_capacity[forecasted6>=max(production),'forecasted6':=max(production)]

# update forecast table
forecast_table[,forecast_with_means_normalized_lag72_dif1:=test_data_means_capacity[date==forecast_date]$forecasted6]
```

With addition of diff1 variable, the model's residual standard error decresead to 0.1295 and adjusted R-squared increased to 0.8943. The diff1 variable is significant as expected. However, the addition of the diff1 variable did not handle the autocorrelation of the residual as much as we expected.

## Model Adjustment 3

Next, to increase the effect of difference, diff24 will be added into the model.

```{r warning=FALSE, message=FALSE}

train_data_means_capacity[,differ24:=train_data_means_capacity$production-shift(train_data_means_capacity$production,24)]
test_data_means_capacity[,differ24:=train_data_means_capacity$production[c(min:max)]-shift(train_data_means_capacity$production[c(min:max)],24)]
train_data_means_capacity$differ24[1:24] = 0
test_data_means_capacity$differ24[1:24] = 0

differ24 <- vector()
control_data_means_capacity <- cbind(control_data_means_capacity, differ24)
control_data_means_capacity$differ24 <- train_data_means_capacity$differ24[evaluation_period-72]

control_data_1week_means_capacity <- cbind(control_data_1week_means_capacity, differ24)
control_data_1week_means_capacity$differ24 <- train_data_means_capacity$differ24[(nrow(train_data_means_capacity)-167):nrow(train_data_means_capacity)]


lm_model7=lm(normalized_production~.,train_data_means_capacity[,-c('date','rolling_max','production','max_prod'),with=F])
summary(lm_model7)
checkresiduals(lm_model7)
lm_forecast_test=predict(lm_model7,test_data_means_capacity)
test_data_means_capacity[,forecasted7:=as.numeric(lm_forecast_test)*rolling_max]
lm_forecast_control=predict(lm_model7,control_data_means_capacity)
control_data_means_capacity[,forecasted7:=as.numeric(lm_forecast_control)*rolling_max]
# for 1 week
lm_forecast_control=predict(lm_model7,control_data_1week_means_capacity)
control_data_1week_means_capacity[,forecasted7:=as.numeric(lm_forecast_control)*rolling_max]

# postprocess the forecast based on actual production
control_data_means_capacity[production<=0,'forecasted7':=0]
control_data_means_capacity[forecasted7<=0,'forecasted7':=0]
control_data_means_capacity[production>=max(production),'forecasted7':=max(production)]
control_data_means_capacity[forecasted7>=max(production),'forecasted7':=max(production)]

control_data_1week_means_capacity[production<=0,'forecasted7':=0]
control_data_1week_means_capacity[forecasted7<=0,'forecasted7':=0]
control_data_1week_means_capacity[production>=max(production),'forecasted7':=max(production)]
control_data_1week_means_capacity[forecasted7>=max(production),'forecasted7':=max(production)]

# update forecast table
forecast_table[,forecast_with_means_normalized_lag72_dif1_dif24:=test_data_means_capacity[date==forecast_date]$forecasted7]

```

Diff24 variable is significant in the model. The addition of the diff24 variable improved the model with the decrease in the residual standard error and increase in the adjusted R-squared value. However, it did not handle the autocorrelation problem. 

## Model Adjustment 4

Next, we decided to add a dummy variable to check outliers in the model which excess the 95% significance level. The reason behind this addition is to accomplish stationarity of the variance and normality of the residuals.

```{r warning=FALSE, message=FALSE}
# outliers exceed 95% significance level removed
train_data_means_capacity[,residuals:=NA] 
train_data_means_capacity$residuals <- lm_model7$residuals[1:(nrow(train_data_means_capacity))]

# Detecting lower and upper 5% residuals and marking them as outlier_small and outlier_great
train_data_means_capacity[!is.na(residuals), quant5:=quantile(residuals,0.05)]
train_data_means_capacity[!is.na(residuals), quant95:=quantile(residuals,0.95)]
train_data_means_capacity[,outlier_small:=as.numeric(residuals<quant5)]
train_data_means_capacity[,outlier_great:=as.numeric(residuals>quant95)]

residual <- vector()
control_data_means_capacity <- cbind(control_data_means_capacity, residuals)
control_data_means_capacity$residuals <- train_data_means_capacity$residuals[evaluation_period-72]

control_data_means_capacity[!is.na(residuals), quant5:=quantile(residuals,0.05)]
control_data_means_capacity[!is.na(residuals), quant95:=quantile(residuals,0.95)]
control_data_means_capacity[,outlier_small:=as.numeric(residuals<quant5)]
control_data_means_capacity[,outlier_great:=as.numeric(residuals>quant95)]

test_data_means_capacity[,outlier_small:=0]
test_data_means_capacity[,outlier_great:=0]

control_data_1week_means_capacity <- cbind(control_data_1week_means_capacity, residuals)
control_data_1week_means_capacity$residuals <- train_data_means_capacity$residuals[(nrow(train_data_means_capacity)-167):nrow(train_data_means_capacity)]

control_data_1week_means_capacity[!is.na(residuals), quant5:=quantile(residuals,0.05)]
control_data_1week_means_capacity[!is.na(residuals), quant95:=quantile(residuals,0.95)]
control_data_1week_means_capacity[,outlier_small:=as.numeric(residuals<quant5)]
control_data_1week_means_capacity[,outlier_great:=as.numeric(residuals>quant95)]

lm_model8=lm(normalized_production~.,train_data_means_capacity[,-c('date','rolling_max','production','max_prod','quant5', 'quant95', 'residuals'),with=F])
summary(lm_model8)
checkresiduals(lm_model8)
lm_forecast_test=predict(lm_model8,test_data_means_capacity)
test_data_means_capacity[,forecasted8:=as.numeric(lm_forecast_test)*rolling_max]
lm_forecast_control=predict(lm_model8,control_data_means_capacity)
control_data_means_capacity[,forecasted8:=as.numeric(lm_forecast_control)*rolling_max]
# for 1 week
lm_forecast_control=predict(lm_model8,control_data_1week_means_capacity)
control_data_1week_means_capacity[,forecasted8:=as.numeric(lm_forecast_control)*rolling_max]

# postprocess the forecast based on actual production
control_data_means_capacity[production<=0,'forecasted8':=0]
control_data_means_capacity[forecasted8<=0,'forecasted8':=0]
control_data_means_capacity[production>=max(production),'forecasted8':=max(production)]
control_data_means_capacity[forecasted8>=max(production),'forecasted8':=max(production)]

control_data_1week_means_capacity[production<=0,'forecasted8':=0]
control_data_1week_means_capacity[forecasted8<=0,'forecasted8':=0]
control_data_1week_means_capacity[production>=max(production),'forecasted8':=max(production)]
control_data_1week_means_capacity[forecasted8>=max(production),'forecasted8':=max(production)]


# update forecast table
forecast_table[,forecast_with_means_normalized_lag72_dif1_dif24_outlier:=test_data_means_capacity[date==forecast_date]$forecasted8]

```

With addition of dummy variables which check outliers, the residual standard error decreased significantly to 0.06912, adjusted R-squared value increased to 0.9699. The dummy variables are both significant. After checking the residuals, the stationary of the variance and normality of the residuals seems better compared to previous models. Considering the all statistics, this is the best model we achieved. Therefore, this model will be used as our final model. 

# 4 - Results

Different from the base model which contains the averages of weather variables, hourly seasonality and normalized production according to capacity, the final model additionally includes lag72, differ1, differ24, outlier_great and outlier_small variables. It is important that all additional variables are statistically significant in the model.

Thanks to added variables, our model performs better in terms of residual standard errors, adjusted R-quared value and weighted mean absolute percentage error. In the base model, the residual standard error was 0.1518 and adjusted R-squared value was 0.855. In the final model, the residual standard error decreased to 0.06884 and adjusted R-squared value increased 0.9703.

Now, lets compare the base model and final model visually:

```{r Comparison of Base and Final Models Visually, warning=FALSE, message=FALSE}
cols <- c("forecasted" = "orange", "actual" = "blue")
ggplot() + 
  geom_line(data = control_data_means_capacity, aes(x = datetime, y = forecasted3,color = "forecasted")) +
  geom_line(data = control_data_means_capacity, aes(x = datetime, y = production,color = "actual")) +
  labs(title = "Predicted vs. Actual for Base Model",
       subtitle = "1 March 2022- 24 May 2022",
       x = "Time",
       y = "Production",
       color = "Color") +
  scale_color_manual(values = cols) +
  scale_x_datetime(breaks = "1 month",expand = c(0,0)) +
  theme_minimal()

ggplot() + 
  geom_line(data = control_data_means_capacity, aes(x = datetime, y = forecasted8,color = "forecasted")) +
  geom_line(data = control_data_means_capacity, aes(x = datetime, y = production,color = "actual")) +
  labs(title = "Predicted vs. Actual for Final Model",
       subtitle = "1 March 2022- 24 May 2022",
       x = "Time",
       y = "Production",
       color = "Color") +
  scale_color_manual(values = cols) +
  scale_x_datetime(breaks = "14 days",expand = c(0,0)) +
  theme_minimal()
```

After checking the two plots, it can be easily observed that our final model explains the available data way better than base model. 

Another way to compare the base and final model is to check statistics such as bias, MAD and WMAPE:

```{r warning=FALSE, message=FALSE}
#reporting accuracy
statistics <- function(actual, forecasted){
  n=length(actual)
  error = actual - forecasted
  mean = mean(actual)
  sd = sd(actual)
  bias = sum(error) / sum(actual)
  mad = sum(abs(error)) / n
  wmape = mad / mean
  l = data.frame(n, mean, sd, bias, mad, wmape)
  colnames(l) <- c("N", "Mean", "Standard Deviation", "Bias", "MAD", "WMAPE")
  return(l)
}

```

```{r warning=FALSE, message=FALSE}
# First Model Statistics for evaluation period
kable(statistics(control_data_means_capacity$production, control_data_means_capacity$forecasted3),
      caption = "Statistics of Base Model for evaluation period ", align = 'c')

# Final Model Statistics for evaluation period
kable(statistics(control_data_means_capacity$production, control_data_means_capacity$forecasted8),
      caption = "Statistics of Final Model for evaluation period", align = 'c')
```

The results tell us that final model is better than base model in terms of all statistics. We managed to decrease our WMAPE result from 0.2415897 to 0.1037525.

Lastly, in order to see the better visualization of how well our final model fit the available data, we will check the actual production vs prediction for the last week of available production data.

```{r warning=FALSE, message=FALSE}
ggplot() + 
  geom_line(data = control_data_1week_means_capacity, aes(x = datetime, y = forecasted3,color = "forecasted")) +
  geom_line(data = control_data_1week_means_capacity, aes(x = datetime, y = production,color = "actual")) +
  labs(title = "Predicted vs. Actual for Base Model",
       subtitle = "Last Week of Available Data",
       x = "Time",
       y = "Production",
       color = "Color") +
  scale_color_manual(values = cols) +
  scale_x_datetime(breaks = "24 hours",expand = c(0,0)) +
  theme_minimal()

ggplot() + 
  geom_line(data = control_data_1week_means_capacity, aes(x = datetime, y = forecasted8,color = "forecasted")) +
  geom_line(data = control_data_1week_means_capacity, aes(x = datetime, y = production,color = "actual")) +
  labs(title = "Predicted vs. Actual for Final Model",
       subtitle = "Last Week of Available Data",
       x = "Time",
       y = "Production",
       color = "Color") +
  scale_color_manual(values = cols) +
  scale_x_datetime(breaks = "24 hours",expand = c(0,0)) +
  theme_minimal()
```

Plots of the last available week of the data represents the model fit better than evaluation period. When the base and final model plots are compared, it can be seen that our final model fit better to the data than base model. However, it is obvious that our final model is not perfect. Even if its residual standard error and WMAPE results are improved, the plot tells us that our forecasted values are somehow unstable during the daytime which distorts the model.   

# 5 - Conclusion and Future Work 

To conclude, even though our model has improved for the forecasting period over time with the additions, it can be seen that the model can be improved further with different approaches. 

Firstly, we did not go into the details of independent weather variables. We followed a data-driven approach and by taking the average values of the different locations and coordinates, we put these variables into our regression models. Therefore, a detailed analysis of these variables can be done and the model can be improved by taking these details into the consideration.   

Next, we did not use ARIMA models in our models and predictions. Therefore, we did not see how such models work in this project. ARIMA models include past errors and previous observations so it can be useful in this project when it is combined with indepenent regressors. 

Moreover, a different data set can be included into this project. There might be other variables to affect the solar power production such as legal regulations, special days, management operational decisions, maintenance periods etc. Therefore, such additions can make the model better. For example, in the competition phase, it can be seen that the last 3 days of the production shows an increasing trend for the production capacity. There  might be reasons behind this increase. 

Therefore, even though we reached a better model than th first model that is used, the model has the space for improvement.  


# 6 - Appendices

* The R Markdown of this report is [here][rmd]. (.Rmd file)
* The R script used for different model trials and making submissions is [here][script]. (.R file)
* Data file used in the project is [here][data]. (.zip file)

# 7 - References

Panjwani, Manoj & Narejo, G.B.. (2014). Effect of humidity on the efficiency of solar cell (Photovoltaic). Int J Eng Res General Sci. 2. 499-503. 

Amusan J. A. & Otokunefor E.B.(2019) The Effect of Cloud on the Output Performance of a Solar Module. IJESC. 9.2. 19665,19671.

<!-- Links -->
[humidity]: https://www.researchgate.net/publication/309104096_Effect_of_humidity_on_the_efficiency_of_solar_cell_photovoltaic "Effect of Humidity on the efficiency of solar cell photovoltaic"
[cloud]: https://ijesc.org/upload/019a8ade10f861b75fa36c98a02d98b9.The%20Effect%20of%20Cloud%20on%20the%20Output%20Performance%20of%20a%20Solar%20Module.pdf "The effect of cloud on the output performance of a solar module"
[rmd]: https://github.com/BU-IE-360/spring22-kutayilmenaytekin/blob/gh-pages/Project/IE360-Project-Report-Group8.Rmd 
[script]: https://github.com/BU-IE-360/spring22-kutayilmenaytekin/blob/gh-pages/Project/ProjectGroup8.R
[data]: https://github.com/BU-IE-360/spring22-kutayilmenaytekin/blob/gh-pages/Project/2022-06-07_production_weather.zip